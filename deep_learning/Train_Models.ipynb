{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:41.960598Z",
     "start_time": "2025-08-07T13:56:41.944966Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:43.862452Z",
     "start_time": "2025-08-07T13:56:42.334453Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, BatchNormalization, LeakyReLU, ELU\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:43.884758Z",
     "start_time": "2025-08-07T13:56:43.863892Z"
    }
   },
   "outputs": [],
   "source": [
    "n_tips = ['674', '489', '87']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:45.041021Z",
     "start_time": "2025-08-07T13:56:43.886157Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_base = '/workspace/deep_birth_death/deep_learning/pickles/simulations/dataset_'\n",
    "data = dict()\n",
    "for i in n_tips:\n",
    "    with open(pickle_base + i + \"_10k.pkl\", 'rb') as f:\n",
    "        data[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:45.070374Z",
     "start_time": "2025-08-07T13:56:45.042639Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_nn(n_out, n_tips, clas=False, div_scenario = None):\n",
    "    input_data = Input(shape=(n_tips, 1))\n",
    "\n",
    "    final_filters = 128\n",
    "    x = Conv1D(16, kernel_size=3, padding='same')(input_data)\n",
    "    x = ELU()(x)\n",
    "    x = Conv1D(16, kernel_size=3, padding='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    if n_tips > 256:\n",
    "        final_filters = 64\n",
    "        x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "        x = ELU()(x)\n",
    "        x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "        if n_tips > 512:\n",
    "            final_filters = 128\n",
    "            x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "            x = ELU()(x)\n",
    "            x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "            x = ELU()(x)\n",
    "            x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(final_filters, kernel_size=3, padding='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(32)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(n_out, name='logits')(x)\n",
    "    if clas:\n",
    "        \n",
    "        output_class = Softmax()(x)\n",
    "        \n",
    "    else:\n",
    "        if div_scenario != \"SAT\":\n",
    "            out_list = []\n",
    "\n",
    "            for i in range(n_out):\n",
    "                y = Dense(32)(x)\n",
    "                y = ELU()(y)\n",
    "                y = Dropout(0.3)(y)\n",
    "\n",
    "                y = Dense(1)(y)\n",
    "                y = LeakyReLU(alpha=10)(y)\n",
    "                out_list.append(y)\n",
    "\n",
    "            output_class = Concatenate()(out_list)\n",
    "            \n",
    "        elif div_scenario == \"WW\":\n",
    "            out_list = []\n",
    "\n",
    "            for i in range(n_out):\n",
    "                y = Dense(32)(x)\n",
    "                y = ELU()(y)\n",
    "                y = Dropout(0.3)(y)\n",
    "\n",
    "                y = Dense(1)(y)\n",
    "                y = Linear(y)\n",
    "                out_list.append(y)\n",
    "\n",
    "            output_class = Concatenate()(out_list)                \n",
    "            \n",
    "        else:\n",
    "            x = Dense(32)(x)\n",
    "            x = ELU()(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(1)(x)\n",
    "            output_class = LeakyReLU(alpha=10)(x)\n",
    "            \n",
    "    return Model(input_data, output_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:56:46.106996Z",
     "start_time": "2025-08-07T13:56:46.079778Z"
    }
   },
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "out_folder_path = \"/workspace/deep_birth_death/deep_learning/models/\"\n",
    "os.makedirs(out_folder_path + 'class/', exist_ok=True)\n",
    "os.makedirs(out_folder_path + 'reg/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T13:09:36.671127Z",
     "start_time": "2025-08-07T12:16:23.245211Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification training 674 tips\n",
      "Elapsed time 923.6486873626709\n",
      "\n",
      "Clasification training 489 tips\n",
      "Elapsed time 1349.151597738266\n",
      "\n",
      "Clasification training 87 tips\n",
      "Elapsed time 920.2880265712738\n"
     ]
    }
   ],
   "source": [
    "for i in n_tips:\n",
    "    print(\"\\nClasification training\", i, 'tips')\n",
    "\n",
    "    nn_model = create_nn(len(data[i]['y_class_train'][0]),\n",
    "                         int(i), clas=True)\n",
    "    nn_model.compile(loss=\"categorical_crossentropy\",\n",
    "                     optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    start = time()\n",
    "    history = nn_model.fit(data[i]['X_train'], data[i]['y_class_train'],\n",
    "                           batch_size=128, epochs=1000, validation_split=0.1,\n",
    "                           callbacks=[callback], verbose=0)\n",
    "    elapsed_time = time()-start\n",
    "    print('Elapsed time', elapsed_time)\n",
    "\n",
    "    save_path = out_folder_path + 'class/' + i + \"_classification_temperature_\"\n",
    "\n",
    "    nn_model.save(save_path + \"model.keras\")\n",
    "    with open(save_path + \"history.pkl\", 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    with open(save_path + \"model_data.pkl\", 'wb') as f:\n",
    "            pickle.dump([nn_model.count_params(), elapsed_time], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:35:31.238429Z",
     "start_time": "2025-08-07T13:56:48.576689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression training 674 tips, BD scenario\n",
      "Elapsed time 87.91219878196716\n",
      "\n",
      "Regression training 674 tips, HE scenario\n",
      "Elapsed time 78.98861718177795\n",
      "\n",
      "Regression training 674 tips, ME scenario\n",
      "Elapsed time 91.36036682128906\n",
      "\n",
      "Regression training 674 tips, SAT scenario\n",
      "Elapsed time 95.16788649559021\n",
      "\n",
      "Regression training 674 tips, SR scenario\n",
      "Elapsed time 110.06941652297974\n",
      "\n",
      "Regression training 674 tips, WW scenario\n",
      "Elapsed time 104.75575184822083\n",
      "\n",
      "Regression training 489 tips, BD scenario\n",
      "Elapsed time 149.27558660507202\n",
      "\n",
      "Regression training 489 tips, HE scenario\n",
      "Elapsed time 100.52225613594055\n",
      "\n",
      "Regression training 489 tips, ME scenario\n",
      "Elapsed time 139.49027013778687\n",
      "\n",
      "Regression training 489 tips, SAT scenario\n",
      "Elapsed time 94.76165795326233\n",
      "\n",
      "Regression training 489 tips, SR scenario\n",
      "Elapsed time 168.76939392089844\n",
      "\n",
      "Regression training 489 tips, WW scenario\n",
      "Elapsed time 175.36655187606812\n",
      "\n",
      "Regression training 87 tips, BD scenario\n",
      "Elapsed time 73.76034808158875\n",
      "\n",
      "Regression training 87 tips, HE scenario\n",
      "Elapsed time 105.3376886844635\n",
      "\n",
      "Regression training 87 tips, ME scenario\n",
      "Elapsed time 166.03365921974182\n",
      "\n",
      "Regression training 87 tips, SAT scenario\n",
      "Elapsed time 93.69172859191895\n",
      "\n",
      "Regression training 87 tips, SR scenario\n",
      "Elapsed time 279.6887767314911\n",
      "\n",
      "Regression training 87 tips, WW scenario\n",
      "Elapsed time 202.9923176765442\n"
     ]
    }
   ],
   "source": [
    "for i in n_tips:\n",
    "    for label in np.unique(data[i]['div_info_train']):\n",
    "        div_scenario = label.split('/')[1].split('_')[0]\n",
    "        #for data_norm in ['_norm']:\n",
    "\n",
    "        # Get regression values of the corresponding scenario\n",
    "        X_train = data[i]['X_train'][data[i]['div_info_train'] == label]\n",
    "        y_reg_train = data[i]['y_reg_norm_train'][data[i]['div_info_train'] == label]\n",
    "        y_reg_train = [np.array(elem) for elem in y_reg_train]\n",
    "        \n",
    "        #Train the model \n",
    "        print(\"\\nRegression training\", i, 'tips,', div_scenario, 'scenario')\n",
    "        nn_model = create_nn(len(y_reg_train[0]),\n",
    "                             int(i), div_scenario=div_scenario)\n",
    "        nn_model.compile(loss=\"mae\", optimizer=Adam(learning_rate=0.001),\n",
    "                         metrics=['mse'])\n",
    "        start = time()\n",
    "        history = nn_model.fit(np.expand_dims(X_train, axis=2),\n",
    "                               np.expand_dims(y_reg_train, axis=2),\n",
    "                               batch_size=128, epochs=1000, validation_split=0.1,\n",
    "                               callbacks=[callback], verbose=0)\n",
    "        elapsed_time = time()-start\n",
    "        print('Elapsed time', elapsed_time)\n",
    "        \n",
    "        #Save the model and the model data\n",
    "        save_path = out_folder_path + 'reg/' + div_scenario + '/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        save_path +=  i + \"_regression\" + '_'\n",
    "        nn_model.save(save_path + \"model.keras\")\n",
    "        with open(save_path + \"history.pkl\", 'wb') as f:\n",
    "                pickle.dump(history.history, f)\n",
    "        with open(save_path + \"model_data.pkl\", 'wb') as f:\n",
    "                pickle.dump([nn_model.count_params(), elapsed_time], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
