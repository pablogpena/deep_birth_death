{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:52:55.367774Z",
     "start_time": "2025-07-03T11:52:55.353131Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:52:57.496896Z",
     "start_time": "2025-07-03T11:52:55.652691Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling1D, MaxPooling1D, Concatenate\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Dropout, BatchNormalization, LeakyReLU, ELU\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:53:25.304239Z",
     "start_time": "2025-07-03T11:53:25.269795Z"
    }
   },
   "outputs": [],
   "source": [
    "n_tips = ['674', '489', '87']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:53:27.452571Z",
     "start_time": "2025-07-03T11:53:26.481013Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_base = '/workspace/phylo_estimation/data_inference/pickles/old_sims/dataset_'\n",
    "data = dict()\n",
    "for i in n_tips:\n",
    "    with open(pickle_base + i + \"_10k.pkl\", 'rb') as f:\n",
    "        data[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:53:28.765589Z",
     "start_time": "2025-07-03T11:53:28.725487Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_nn(n_out, n_tips, clas=False, div_scenario = None):\n",
    "    input_data = Input(shape=(n_tips, 1))\n",
    "\n",
    "    final_filters = 128\n",
    "    x = Conv1D(16, kernel_size=3, padding='same')(input_data)\n",
    "    x = ELU()(x)\n",
    "    x = Conv1D(16, kernel_size=3, padding='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    if n_tips > 256:\n",
    "        final_filters = 64\n",
    "        x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "        x = ELU()(x)\n",
    "        x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "        if n_tips > 512:\n",
    "            final_filters = 128\n",
    "            x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "            x = ELU()(x)\n",
    "            x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "            x = ELU()(x)\n",
    "            x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = Conv1D(final_filters, kernel_size=3, padding='same')(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Dense(32)(x)\n",
    "    x = ELU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(n_out, name='logits')(x)\n",
    "    if clas:\n",
    "        \n",
    "        output_class = Softmax()(x)\n",
    "        \n",
    "    else:\n",
    "        if div_scenario != \"SAT\":\n",
    "            out_list = []\n",
    "\n",
    "            for i in range(n_out):\n",
    "                y = Dense(32)(x)\n",
    "                y = ELU()(y)\n",
    "                y = Dropout(0.3)(y)\n",
    "\n",
    "                y = Dense(1)(y)\n",
    "                y = LeakyReLU(alpha=10)(y)\n",
    "                out_list.append(y)\n",
    "\n",
    "            output_class = Concatenate()(out_list)\n",
    "            \n",
    "        elif div_scenario == \"WW\":\n",
    "            out_list = []\n",
    "\n",
    "            for i in range(n_out):\n",
    "                y = Dense(32)(x)\n",
    "                y = ELU()(y)\n",
    "                y = Dropout(0.3)(y)\n",
    "\n",
    "                y = Dense(1)(y)\n",
    "                y = Linear(y)\n",
    "                out_list.append(y)\n",
    "\n",
    "            output_class = Concatenate()(out_list)                \n",
    "            \n",
    "        else:\n",
    "            x = Dense(32)(x)\n",
    "            x = ELU()(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(1)(x)\n",
    "            output_class = LeakyReLU(alpha=10)(x)\n",
    "            \n",
    "    return Model(input_data, output_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T11:53:30.326390Z",
     "start_time": "2025-07-03T11:53:30.292595Z"
    }
   },
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "out_folder_path = \"/workspace/phylo_estimation/data_inference/models/\"\n",
    "os.makedirs(out_folder_path + 'class/', exist_ok=True)\n",
    "os.makedirs(out_folder_path + 'reg/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T12:34:45.334184Z",
     "start_time": "2025-07-03T11:53:32.658260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasification training 489 tips\n",
      "Elapsed time 1480.9674112796783\n",
      "\n",
      "Clasification training 87 tips\n",
      "Elapsed time 990.7275106906891\n"
     ]
    }
   ],
   "source": [
    "for i in n_tips:\n",
    "    print(\"\\nClasification training\", i, 'tips')\n",
    "\n",
    "    nn_model = create_nn(len(data[i]['y_class_train'][0]),\n",
    "                         int(i), clas=True)\n",
    "    nn_model.compile(loss=\"categorical_crossentropy\",\n",
    "                     optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "    start = time()\n",
    "    history = nn_model.fit(data[i]['X_train'], data[i]['y_class_train'],\n",
    "                           batch_size=128, epochs=1000, validation_split=0.1,\n",
    "                           callbacks=[callback], verbose=0)\n",
    "    elapsed_time = time()-start\n",
    "    print('Elapsed time', elapsed_time)\n",
    "\n",
    "    save_path = out_folder_path + 'class/' + i + \"_classification_temperature\"\n",
    "\n",
    "    nn_model.save(save_path + \"model.keras\")\n",
    "    with open(save_path + \"history.pkl\", 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    with open(save_path + \"model_data.pkl\", 'wb') as f:\n",
    "            pickle.dump([nn_model.count_params(), elapsed_time], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-01T17:36:04.416Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression training 674 tips, BD model, _norm norm\n",
      "Elapsed time 74.6568374633789\n",
      "Errors\n",
      "0.0011301899303287422\n",
      "0.008952771129352227\n",
      "Total error: 0.005041480529840484\n",
      "Denorm Errors\n",
      "0.0331025984384714\n",
      "0.24545766634449478\n",
      "Total error: 0.1392801323914831\n",
      "\n",
      "Regression training 674 tips, HE model, _norm norm\n",
      "Elapsed time 83.99164414405823\n",
      "Errors\n",
      "0.0004357095942337175\n",
      "0.0005123358196911457\n",
      "Total error: 0.0004740227069624315\n",
      "Denorm Errors\n",
      "0.09490874863614153\n",
      "0.11000810019870627\n",
      "Total error: 0.10245842441742391\n",
      "\n",
      "Regression training 674 tips, ME model, _norm norm\n",
      "Elapsed time 114.39358472824097\n",
      "Errors\n",
      "0.003464777570768988\n",
      "0.0070273620405777535\n",
      "129.14969196397666\n",
      "0.003318942803083203\n",
      "Total error: 32.29087576159777\n",
      "Denorm Errors\n",
      "0.0023229887312050733\n",
      "0.007010279257976845\n",
      "54.660379851262526\n",
      "0.0036846439847519103\n",
      "Total error: 13.668349440809113\n",
      "\n",
      "Regression training 674 tips, SAT model, _norm norm\n",
      "Elapsed time 87.18519997596741\n",
      "Errors\n",
      "0.023596867233495047\n",
      "Total error: 0.023596867233495047\n",
      "Denorm Errors\n",
      "0.008908373878321862\n",
      "Total error: 0.008908373878321862\n",
      "\n",
      "Regression training 674 tips, SR model, _norm norm\n",
      "Elapsed time 102.42545175552368\n",
      "Errors\n",
      "0.01976220335928931\n",
      "0.0008209116710695729\n",
      "0.016818109791881496\n",
      "0.012115764387870964\n",
      "36.814618979577396\n",
      "Total error: 7.3728271937575025\n",
      "Denorm Errors\n",
      "0.008042070042594072\n",
      "0.0010126690101373197\n",
      "0.029398949278035248\n",
      "0.022437407476977856\n",
      "32.15150429817589\n",
      "Total error: 6.442479078796728\n",
      "\n",
      "Regression training 674 tips, WW model, _norm norm\n",
      "Elapsed time 120.57813763618469\n",
      "Errors\n",
      "0.031077913476269595\n",
      "0.3786838529298906\n",
      "0.03279199941606649\n",
      "0.010261312451502077\n",
      "336.76536858733965\n",
      "Total error: 67.44363673312266\n",
      "Denorm Errors\n",
      "0.0016847267984807638\n",
      "0.01856868446010464\n",
      "0.0028879767496671296\n",
      "0.0008900089981963701\n",
      "15.56420661888314\n",
      "Total error: 3.117647603177918\n",
      "\n",
      "Regression training 489 tips, BD model, _norm norm\n",
      "Elapsed time 130.90232825279236\n",
      "Errors\n",
      "0.0012731311397394974\n",
      "0.008958512538950138\n",
      "Total error: 0.005115821839344817\n",
      "Denorm Errors\n",
      "0.04012009853525769\n",
      "0.28228149573814143\n",
      "Total error: 0.16120079713669955\n",
      "\n",
      "Regression training 489 tips, HE model, _norm norm\n",
      "Elapsed time 99.37144064903259\n",
      "Errors\n",
      "0.0005198732645676532\n",
      "0.0005443982503339017\n",
      "Total error: 0.0005321357574507773\n",
      "Denorm Errors\n",
      "0.11275985319473818\n",
      "0.11720484098700636\n",
      "Total error: 0.11498234709087228\n",
      "\n",
      "Regression training 489 tips, ME model, _norm norm\n",
      "Elapsed time 143.39661145210266\n",
      "Errors\n",
      "0.002840352863768968\n",
      "0.007774652494934296\n",
      "120.21797536901087\n",
      "0.0032028999215803007\n",
      "Total error: 30.05794831857279\n",
      "Denorm Errors\n",
      "0.002215163829375043\n",
      "0.007738968535712208\n",
      "66.23721523655992\n",
      "0.0034778925140734242\n",
      "Total error: 16.562661815359775\n",
      "\n",
      "Regression training 489 tips, SAT model, _norm norm\n",
      "Elapsed time 121.06200218200684\n",
      "Errors\n",
      "0.02573097580718247\n",
      "Total error: 0.02573097580718247\n",
      "Denorm Errors\n",
      "0.010969759883234035\n",
      "Total error: 0.010969759883234035\n",
      "\n",
      "Regression training 489 tips, SR model, _norm norm\n"
     ]
    }
   ],
   "source": [
    "for i in n_tips:\n",
    "    for label in np.unique(data[i]['div_info_train']):\n",
    "        div_scenario = label.split('/')[1].split('_')[0]\n",
    "        for data_norm in ['_norm']:\n",
    "\n",
    "            # Get regression values of the corresponding scenario\n",
    "            X_train = data[i]['X_train'][data[i]['div_info_train'] == label]\n",
    "            y_reg_train = data[i]['y_reg' + data_norm + '_train'][data[i]['div_info_train'] == label]\n",
    "            y_reg_train = [np.array(elem) for elem in y_reg_train]\n",
    "            \n",
    "            resc_factor_train = data[i]['resc_factor_train'][data[i]['div_info_train'] == label]\n",
    "            \n",
    "            print(\"\\nRegression training\", i, 'tips,', div_scenario, 'model,', data_norm, 'norm')\n",
    "            nn_model = create_nn(len(y_reg_train[0]),\n",
    "                                 int(i), div_scenario=div_scenario)\n",
    "            nn_model.compile(loss=\"mae\", optimizer=Adam(learning_rate=0.001),\n",
    "                             metrics=['mse'])\n",
    "\n",
    "            start = time()\n",
    "            history = nn_model.fit(np.expand_dims(X_train, axis=2),\n",
    "                                   np.expand_dims(y_reg_train, axis=2),\n",
    "                                   batch_size=128, epochs=1000, validation_split=0.1,\n",
    "                                   callbacks=[callback], verbose=0)\n",
    "            elapsed_time = time()-start\n",
    "            print('Elapsed time', elapsed_time)\n",
    "\n",
    "            save_path = out_folder_path + 'reg/' + div_scenario + '/'\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            save_path +=  i + \"_regression\" + data_norm + '_'\n",
    "\n",
    "            nn_model.save(save_path + \"model.keras\")\n",
    "            with open(save_path + \"history.pkl\", 'wb') as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "            with open(save_path + \"model_data.pkl\", 'wb') as f:\n",
    "                    pickle.dump([nn_model.count_params(), elapsed_time], f)\n",
    "                    \n",
    "            pred = nn_model.predict(np.expand_dims(data[i]['X_test'][data[i]['div_info_test'] == label], axis=2))\n",
    "            y_reg_test = data[i]['y_reg' + data_norm + '_test'][data[i]['div_info_test'] == label]\n",
    "            y_reg_test = [np.array(elem) for elem in y_reg_test]\n",
    "            resc_factor_test = data[i]['resc_factor_test'][data[i]['div_info_test'] == label]\n",
    "\n",
    "            real = y_reg_test\n",
    "            error = (pred-real)**2\n",
    "            \n",
    "            print('Errors')\n",
    "            for j in range(len(pred[0])):\n",
    "                print(np.mean(error[:,j]))\n",
    "            print('Total error:', np.mean(error))\n",
    "            \n",
    "            pred/=resc_factor_test[:, np.newaxis]\n",
    "            real = y_reg_test\n",
    "            real/=resc_factor_test[:, np.newaxis]\n",
    "            error = (pred-real)**2\n",
    "            print('Denorm Errors')\n",
    "            for j in range(len(pred[0])):\n",
    "                print(np.mean(error[:,j]))\n",
    "            print('Total error:', np.mean(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
